<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="论文,实体识别,关系抽取," />










<meta name="description" content="信息抽取引言2017-2018 基于神经网络的实体识别和关系抽取联合学习 本文关注的任务是从无结构的文本中抽取实体以及实体之间的关系（实体1-关系-实体2，三元组），这里的关系是我们预定义好的关系类型。例如下图， Multiple relations extraction among multiple entities in unstructured text  To the best of ou">
<meta name="keywords" content="论文,实体识别,关系抽取">
<meta property="og:type" content="article">
<meta property="og:title" content="信息抽取任务相关论文发展脉络">
<meta property="og:url" content="http://yoursite.com/2019/05/28/信息抽取任务相关论文发展脉络/index.html">
<meta property="og:site_name" content="望江人工智库">
<meta property="og:description" content="信息抽取引言2017-2018 基于神经网络的实体识别和关系抽取联合学习 本文关注的任务是从无结构的文本中抽取实体以及实体之间的关系（实体1-关系-实体2，三元组），这里的关系是我们预定义好的关系类型。例如下图， Multiple relations extraction among multiple entities in unstructured text  To the best of ou">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/05/28/信息抽取任务相关论文发展脉络/example.png">
<meta property="og:image" content="http://yoursite.com/2019/05/28/信息抽取任务相关论文发展脉络/model-2018Bekoulis.png">
<meta property="og:image" content="http://yoursite.com/2019/05/28/信息抽取任务相关论文发展脉络/model-2018Bekoulis-2.png">
<meta property="og:image" content="http://yoursite.com/2019/05/28/信息抽取任务相关论文发展脉络/model-2018Verga.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/2412555/54099204-25717680-440c-11e9-8673-5ecdb05b25e1.png">
<meta property="og:image" content="http://yoursite.com/2019/05/28/信息抽取任务相关论文发展脉络/model-2019Yan.png">
<meta property="og:image" content="http://yoursite.com/2019/05/28/信息抽取任务相关论文发展脉络/model-2019Wang.png">
<meta property="og:image" content="http://yoursite.com/2019/05/28/信息抽取任务相关论文发展脉络/illustration-2019Wang.png">
<meta property="og:updated_time" content="2019-06-25T02:45:26.816Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="信息抽取任务相关论文发展脉络">
<meta name="twitter:description" content="信息抽取引言2017-2018 基于神经网络的实体识别和关系抽取联合学习 本文关注的任务是从无结构的文本中抽取实体以及实体之间的关系（实体1-关系-实体2，三元组），这里的关系是我们预定义好的关系类型。例如下图， Multiple relations extraction among multiple entities in unstructured text  To the best of ou">
<meta name="twitter:image" content="http://yoursite.com/2019/05/28/信息抽取任务相关论文发展脉络/example.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/05/28/信息抽取任务相关论文发展脉络/"/>





  <title>信息抽取任务相关论文发展脉络 | 望江人工智库</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
	<a href="https://github.com/yuanxiaosc" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">望江人工智库</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/28/信息抽取任务相关论文发展脉络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="望江车神">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars0.githubusercontent.com/u/16183570?s=400&u=5e09ebb784cfd47de99d249f2be2413adcf4e672&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="望江人工智库">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">信息抽取任务相关论文发展脉络</h1>
        

        <div class="post-meta">
		  

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-28T15:30:00+08:00">
                2019-05-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文/" itemprop="url" rel="index">
                    <span itemprop="name">论文</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文/信息抽取/" itemprop="url" rel="index">
                    <span itemprop="name">信息抽取</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/28/信息抽取任务相关论文发展脉络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/05/28/信息抽取任务相关论文发展脉络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="信息抽取引言"><a href="#信息抽取引言" class="headerlink" title="信息抽取引言"></a>信息抽取引言</h3><p><a href="https://www.cnblogs.com/robert-dlut/p/7710735.html" target="_blank" rel="noopener">2017-2018 基于神经网络的实体识别和关系抽取联合学习</a></p>
<p>本文关注的任务是从无结构的文本中抽取实体以及实体之间的关系（实体1-关系-实体2，三元组），这里的关系是我们预定义好的关系类型。例如下图，<br><img src="/2019/05/28/信息抽取任务相关论文发展脉络/example.png" alt=""></p>
<h4 id="Multiple-relations-extraction-among-multiple-entities-in-unstructured-text"><a href="#Multiple-relations-extraction-among-multiple-entities-in-unstructured-text" class="headerlink" title="Multiple relations extraction among multiple entities in unstructured text"></a><a href="https://link.springer.com/article/10.1007%2Fs00500-017-2852-8" target="_blank" rel="noopener">Multiple relations extraction among multiple entities in unstructured text</a></h4><blockquote>
<p> To the best of our knowledge, our proposed model is the ﬁrst one to extract multiple relations among multiple entities.</p>
<p>Relations extraction is a widely researched topic in nature language processing. However, most of the work in the literature concentrate on the methods that are dealing with single relation between two named entities. In the task of multiple relations extraction, traditional statistic-based methods have difficulties in selecting features and improving the performance of extraction model. In this paper, we presented formal definitions of multiple entities and multiple relations and put forward three labeling methods which were used to label entity categories, relation categories and relation conditions. We also proposed a novel relation extraction model which is based on dynamic long short-term memory network. To train our model, entity feature, entity position feature and part of speech feature are used together. These features are used to describe complex relations and improve the performance of relation extraction model. In the experiments, we classified the corpus into three sets which are composed of 0–20 words, 20–35 words and 35+ words sentences. On conll04.corp, the final precision, recall rate and F-measure reached 72.9, 70.8 and 67.9% respectively.</p>
<p>关系提取是自然语言处理中广泛研究的主题。但是，文献中的大多数工作都集中在处理两个命名实体之间单一关系的方法上。在多关系提取的任务中，传统的基于统计的方法在选择特征和提高提取模型的性能方面存在困难。在本文中，我们提出了多个实体和多个关系的正式定义，并提出了三种标记方法，用于标记实体类别，关系类别和关系条件。我们还提出了一种基于动态长短期记忆网络的新型关系提取模型。为了训练我们的模型，实体特征，实体位置特征和词性特征一起使用。这些特征用于描述复杂关系并改善关系提取模型的性能。在实验中，我们将语料库分为三组，由0-20个单词，20-35个单词和35个单词组成。在conll04.corp上，最终的精确度，召回率和F-指数分别达到72.9％，70.8％和67.9％。</p>
</blockquote>
<h4 id="Joint-entity-recognition-and-relation-extraction-as-a-multi-head-selection-problem"><a href="#Joint-entity-recognition-and-relation-extraction-as-a-multi-head-selection-problem" class="headerlink" title="Joint entity recognition and relation extraction as a multi-head selection problem"></a><a href="https://arxiv.org/abs/1804.07847" target="_blank" rel="noopener">Joint entity recognition and relation extraction as a multi-head selection problem</a></h4><p><strong>Abstract</strong></p>
<blockquote>
<p>State-of-the-art models for joint entity recognition and relation extraction strongly rely on external natural language processing (NLP) tools such as POS (part-of-speech) taggers and dependency parsers. Thus, the performance of such joint models depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various languages and contexts. In this paper, we propose a joint neural model which performs entity recognition and relation extraction simultaneously, without the need of any manually extracted features or the use of any external tool. Specifically, we model the entity recognition task using a CRF (Conditional Random Fields) layer and the relation extraction task as a multi-head selection problem (i.e., potentially identify multiple relations for each entity). We present an extensive experimental setup, to demonstrate the effectiveness of our method using datasets from various contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). Our model outperforms the previous neural models that use automatically extracted features, while it performs within a reasonable margin of feature-based neural models, or even beats them.</p>
</blockquote>
<p><strong>摘要</strong></p>
<blockquote>
<p>用于联合实体识别和关系提取的最先进模型强烈依赖于外部自然语言处理（NLP）工具，例如POS（词性）标记器和依赖性解析器。因此，这种联合模型的性能取决于从这些NLP工具获得的特征的质量。但是，对于各种语言和上下文，这些功能并不总是准确的。在本文中，我们提出了一个联合神经模型，它同时执行实体识别和关系提取，无需任何手动提取的功能或使用任何外部工具。具体地，我们使用CRF（条件随机场）层和关系提取任务将实体识别任务建模为多头选择问题（即，潜在地识别每个实体的多个关系）。我们提出了一个广泛的实验设置，以证明我们的方法使用来自各种环境（即新闻，生物医学，房地产）和语言（即英语，荷兰语）的数据集的有效性。我们的模型优于以前使用自动提取特征的神经模型，同时它在基于特征的神经模型的合理范围内执行，甚至胜过它们。</p>
</blockquote>
<p><img src="/2019/05/28/信息抽取任务相关论文发展脉络/model-2018Bekoulis.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://arxiv.org/abs/1804.07847" target="_blank" rel="noopener">Joint entity recognition and relation extraction as a multi-head selection problem</a></td>
<td>论文原文</td>
<td>2018</td>
</tr>
<tr>
<td><a href="https://github.com/bekou/multihead_joint_entity_relation_extraction" target="_blank" rel="noopener">multihead_joint_entity_relation_extraction</a></td>
<td>论文实现</td>
<td>20190429</td>
</tr>
<tr>
<td><a href="https://zhuanlan.zhihu.com/p/46507254" target="_blank" rel="noopener">对抗训练多头选择的实体识别和关系抽取的联合模型</a></td>
<td>论文解析</td>
<td>20181005</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Adversarial-training-for-multi-context-joint-entity-and-relation-extraction"><a href="#Adversarial-training-for-multi-context-joint-entity-and-relation-extraction" class="headerlink" title="Adversarial training for multi-context joint entity and relation extraction"></a><a href="https://arxiv.org/abs/1808.06876" target="_blank" rel="noopener">Adversarial training for multi-context joint entity and relation extraction</a></h4><blockquote>
<p>本文是 <a href="https://arxiv.org/abs/1804.07847" target="_blank" rel="noopener">Joint entity recognition and relation extraction as a multi-head selection problem</a> 的姊妹篇，都是同一个多头选择实体关系抽取模型，只是增加了对抗训练这个神经网络正则化方法。</p>
</blockquote>
<p><strong>Abstract</strong></p>
<blockquote>
<p>Adversarial training (AT) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data. We show how to use AT for the tasks of entity recognition and relation extraction. In particular, we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations, allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e., news, biomedical, and real estate data) and for different languages (English and Dutch).</p>
</blockquote>
<p><strong>摘要</strong></p>
<blockquote>
<p>对抗训练（AT）是一种正则化方法，可以通过在训练数据中添加小扰动来提高神经网络方法的鲁棒性。 我们展示了如何将AT用于实体识别和关系提取的任务。 特别是，我们证明将AT应用于通用基线模型以联合提取实体和关系，可以改善不同环境中几个数据集（即新闻，生物医学和房地产数据）的最新效果。 和不同的语言（英语和荷兰语）。</p>
</blockquote>
<p><img src="/2019/05/28/信息抽取任务相关论文发展脉络/model-2018Bekoulis-2.png" alt=""></p>
<p><strong>3.2 Adversarial traing (AT)</strong><br>We exploit the idea of AT (Goodfellow et al., 2015) as a regularization method to make our model robust to input perturbations. Speciﬁcally, we generate examples which are variations of the original ones by adding some noise at the level of the concatenated word representation (Miyato et al., 2017). This is similar to the concept introduced by Goodfellow et al. (2015) to improve the robustness of image recognition classiﬁers. We generate an adversarial example by adding the worst-case perturbation $\eta_{adv}$ to the original embedding w that maximizes the loss function:</p>
<script type="math/tex; mode=display">\eta_{adv}=argmaxLoss(w+\eta;\hat\theta),\Vert\eta\Vert\leq\epsilon</script><p>where $\hat\theta$ is a copy of the current model parameters. Since Eq. (2) is intractable in neural networks, we use the approximation proposed in Goodfellow et al. (2015) deﬁned as: $\eta_{adv}=\epsilon g/\Vert g \Vert$, with $g=\nabla_wLoss(w;\hat\theta)$, where $\epsilon$ is a small bounded norm treated as a hyperparameter. Similar to Yasunaga et al. (2018), we set $\epsilon$ to be $\alpha\sqrt{D}$ (where $D$ is the dimension of the embeddings). We train on the mixture of original and adversarial examples, so the ﬁnal loss is computed as: $Loss(w;\hat\theta)+Loss(w+\eta;\hat\theta)$.</p>
<script type="math/tex; mode=display">\eta_{adv}=\epsilon g/\Vert g \Vert, g=\nabla_wLoss(w;\hat\theta),\epsilon=\alpha\sqrt{D}</script><div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://arxiv.org/abs/1808.06876" target="_blank" rel="noopener">Adversarial training for multi-context joint entity and relation extraction</a></td>
<td>论文原文</td>
<td>2018</td>
</tr>
<tr>
<td><a href="https://github.com/bekou/multihead_joint_entity_relation_extraction" target="_blank" rel="noopener">multihead_joint_entity_relation_extraction</a></td>
<td>论文实现</td>
<td>20190429</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Simultaneously-Self-Attending-to-All-Mentions-for-Full-Abstract-Biological-Relation-Extraction"><a href="#Simultaneously-Self-Attending-to-All-Mentions-for-Full-Abstract-Biological-Relation-Extraction" class="headerlink" title="Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction"></a><a href="https://aclweb.org/anthology/papers/N/N18/N18-1080/" target="_blank" rel="noopener">Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction</a></h4><p><strong>Abstract</strong></p>
<blockquote>
<p>Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.</p>
</blockquote>
<p><strong>摘要</strong></p>
<blockquote>
<p>关系提取中的大多数工作通过查看包含单个实体对提及的单个句子中的短文本来形成预测。这种方法通常不考虑提及的交互，需要对每个提及对进行冗余计算，并忽略跨句子边界表达的关系。生物文本中常见的文档（而不是句子）级别注释加剧了这些问题。作为回应，我们提出了一个模型，它同时预测文档中所有提及对之间的关​​系。我们使用有效的自我关注编码器在整个纸质摘要上形成成对预测。所有对提及分数允许我们通过聚合提及以形成实体对表示来执行多实例学习。我们通过联合训练来预测命名实体并添加弱标记数据的语料库，从而进一步适应没有提及级别注释的设置。在两个Biocreative基准数据集的实验中，我们在Biocreative V Chemical Disease Relation数据集上获得了没有外部KB资源的模型的最新性能。我们还引入了一个比现有的人类注释生物信息提取数据集大一个数量级的新数据集，并且比远程监督的替代方案更准确。</p>
</blockquote>
<p><img src="/2019/05/28/信息抽取任务相关论文发展脉络/model-2018Verga.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://aclweb.org/anthology/papers/N/N18/N18-1080/" target="_blank" rel="noopener">Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction</a></td>
<td>论文原文</td>
<td>2018</td>
</tr>
</tbody>
</table>
</div>
<h4 id="End-to-end-neural-relation-extraction-using-deep-biaffine-attention"><a href="#End-to-end-neural-relation-extraction-using-deep-biaffine-attention" class="headerlink" title="End-to-end neural relation extraction using deep biaffine attention"></a><a href="https://arxiv.org/abs/1812.11275" target="_blank" rel="noopener">End-to-end neural relation extraction using deep biaffine attention</a></h4><blockquote>
<p>We propose a neural network model for joint extraction of named entities and relations between them, without any hand-crafted features. The key contribution of our model is to extend a BiLSTM-CRF-based entity recognition model with a deep biaffine attention layer to model second-order interactions between latent features for relation classification, specifically attending to the role of an entity in a directional relationship. On the benchmark “relation and entity recognition” dataset CoNLL04, experimental results show that our model outperforms previous models, producing new state-of-the-art performances.</p>
<p>我们提出了一种神经网络模型，用于联合提取命名实体及其之间的关系，没有任何手工制作的特征。 我们模型的关键贡献是扩展基于BiLSTM-CRF的实体识别模型，该模型具有深的biaffine注意层，以模拟关系分类的潜在特征之间的二阶相互作用，特别是关注实体在方向关系中的角色。 在基准“关系和实体识别”数据集CoNLL04上，实验结果表明我们的模型优于以前的模型，产生了新的最先进的性能。</p>
</blockquote>
<p><img src="https://user-images.githubusercontent.com/2412555/54099204-25717680-440c-11e9-8673-5ecdb05b25e1.png" alt=""></p>
<p><a href="https://github.com/datquocnguyen/jointRE" target="_blank" rel="noopener">jointRE</a></p>
<h4 id="A-Unified-Model-for-Joint-Chinese-Word-Segmentation-and-Dependency-Parsing"><a href="#A-Unified-Model-for-Joint-Chinese-Word-Segmentation-and-Dependency-Parsing" class="headerlink" title="A Unified Model for Joint Chinese Word Segmentation and Dependency Parsing"></a><a href="https://arxiv.org/abs/1904.04697" target="_blank" rel="noopener">A Unified Model for Joint Chinese Word Segmentation and Dependency Parsing</a></h4><blockquote>
<p>Chinese word segmentation and dependency parsing are two fundamental tasks for Chinese natural language processing. The dependency parsing is defined on word-level, therefore word segmentation is the precondition of dependency parsing, which makes dependency parsing suffers from error propagation. In this paper, we propose a unified model to integrate Chinese word segmentation and dependency parsing. Different from previous joint models, our proposed model is a graph-based model and more concise, which results in fewer efforts of feature engineering. Our joint model achieves better performance than previous joint models. Our joint model achieves the state-of-the-art results in both Chinese word segmentation and dependency parsing.</p>
<p>中文分词和依赖解析是中文自然语言处理的两个基本任务。 依赖性解析是在字级别定义的，因此分词是依赖性解析的前提条件，这使得依赖性解析受到错误传播的影响。 在本文中，我们提出了一个统一的模型来集成中文分词和依赖解析。 与以前的联合模型不同，我们提出的模型是基于图形的模型，更简洁，从而减少了特征工程的工作量。 我们的联合模型比以前的联合模型具有更好的性能。 我们的联合模型在中文分词和依赖分析中实现了最先进的结果。</p>
</blockquote>
<p><img src="/2019/05/28/信息抽取任务相关论文发展脉络/model-2019Yan.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://arxiv.org/abs/1904.04697" target="_blank" rel="noopener">A Unified Model for Joint Chinese Word Segmentation and Dependency Parsing</a></td>
<td>论文原文</td>
<td>20190409</td>
</tr>
<tr>
<td><a href="https://www.jiqizhixin.com/articles/2019-04-27" target="_blank" rel="noopener">联合汉语分词和依存句法分析的统一模型：当前效果最佳</a></td>
<td>论文浅析</td>
<td>20190427</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Extracting-Multiple-Relations-in-One-Pass-with-Pre-Trained-Transformers"><a href="#Extracting-Multiple-Relations-in-One-Pass-with-Pre-Trained-Transformers" class="headerlink" title="Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers"></a><a href="https://arxiv.org/abs/1902.01030" target="_blank" rel="noopener">Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers</a></h4><blockquote>
<p>this work is the ﬁrst approach that can solve  multi-relation extraction (MRE) task accurately to achieve state-of-the-art results and  fast  (in one-pass).</p>
</blockquote>
<p><strong>Abstract</strong></p>
<blockquote>
<p>Most approaches to extraction multiple relations from a paragraph require multiple passes over the paragraph. In practice, multiple passes are computationally expensive and this makes difficult to scale to longer paragraphs and larger text corpora. In this work, we focus on the task of multiple relation extraction by encoding the paragraph only once (one-pass). We build our solution on the pre-trained self-attentive (Transformer) models, where we first add a structured prediction layer to handle extraction between multiple entity pairs, then enhance the paragraph embedding to capture multiple relational information associated with each entity with an entity-aware attention technique. We show that our approach is not only scalable but can also perform state-of-the-art on the standard benchmark ACE 2005.</p>
</blockquote>
<p><strong>摘要</strong></p>
<blockquote>
<p>从段落中提取多个关系的大多数方法都要求在段落上多次传递。 在实践中，多次通过在计算上是昂贵的，并且这使得难以扩展到更长的段落和更大的文本语料库。 在这项工作中，我们通过仅对段落进行一次编码（一次通过）来专注于多关系提取的任务。 我们在预训练的自我关注（Transformer）模型上构建我们的解决方案，我们首先添加结构化预测层来处理多个实体对之间的提取，然后增强段落嵌入以捕获与每个实体关联的多个关系信息与实体  - 注意技术。 我们表明，我们的方法不仅可扩展，而且还可以在标准基准ACE 2005上执行最先进的技术。</p>
</blockquote>
<p><img src="/2019/05/28/信息抽取任务相关论文发展脉络/model-2019Wang.png" alt=""><br><img src="/2019/05/28/信息抽取任务相关论文发展脉络/illustration-2019Wang.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://arxiv.org/abs/1902.01030" target="_blank" rel="noopener">Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers</a></td>
<td>论文原文</td>
</tr>
</tbody>
</table>
</div>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>感谢金主！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="望江车神 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="望江车神 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/论文/" rel="tag"># 论文</a>
          
            <a href="/tags/实体识别/" rel="tag"># 实体识别</a>
          
            <a href="/tags/关系抽取/" rel="tag"># 关系抽取</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/05/17/多关系抽取研究/" rel="next" title="多关系抽取研究">
                <i class="fa fa-chevron-left"></i> 多关系抽取研究
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/04/EfficientNet_Rethinking_Model_Scaling_for_Convolutional_Neural_Networks/" rel="prev" title="EfficientNet Rethinking Model Scaling for Convolutional Neural Networks">
                EfficientNet Rethinking Model Scaling for Convolutional Neural Networks <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars0.githubusercontent.com/u/16183570?s=400&u=5e09ebb784cfd47de99d249f2be2413adcf4e672&v=4"
                alt="望江车神" />
            
              <p class="site-author-name" itemprop="name">望江车神</p>
              <p class="site-description motion-element" itemprop="description">深度学习你~~~</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">141</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">57</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">126</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yuanxiaoSC" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:wangzichaochaochao@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#信息抽取引言"><span class="nav-number">1.</span> <span class="nav-text">信息抽取引言</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiple-relations-extraction-among-multiple-entities-in-unstructured-text"><span class="nav-number">1.1.</span> <span class="nav-text">Multiple relations extraction among multiple entities in unstructured text</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Joint-entity-recognition-and-relation-extraction-as-a-multi-head-selection-problem"><span class="nav-number">1.2.</span> <span class="nav-text">Joint entity recognition and relation extraction as a multi-head selection problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adversarial-training-for-multi-context-joint-entity-and-relation-extraction"><span class="nav-number">1.3.</span> <span class="nav-text">Adversarial training for multi-context joint entity and relation extraction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Simultaneously-Self-Attending-to-All-Mentions-for-Full-Abstract-Biological-Relation-Extraction"><span class="nav-number">1.4.</span> <span class="nav-text">Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#End-to-end-neural-relation-extraction-using-deep-biaffine-attention"><span class="nav-number">1.5.</span> <span class="nav-text">End-to-end neural relation extraction using deep biaffine attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-Unified-Model-for-Joint-Chinese-Word-Segmentation-and-Dependency-Parsing"><span class="nav-number">1.6.</span> <span class="nav-text">A Unified Model for Joint Chinese Word Segmentation and Dependency Parsing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Extracting-Multiple-Relations-in-One-Pass-with-Pre-Trained-Transformers"><span class="nav-number">1.7.</span> <span class="nav-text">Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">望江车神</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'aTXvwHFSoz68yg6g3k5JzN7B-MdYXbMMI',
        appKey: 'Wkf7bKVEfcQ0sW4V1l144HLY',
        placeholder: '欢迎交流',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
